{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from sklearn import preprocessing, decomposition, manifold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#3d plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib qt\n",
    "#plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather data (.csv) produced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data as pandas dataframe\n",
    "path_to_features = \"./datasets/bach_features.csv\"\n",
    "path_to_ratings = \"./datasets/bach_ratings.csv\"\n",
    "data = pd.read_csv(path_to_features)\n",
    "rating = pd.read_csv(path_to_ratings)\n",
    "\n",
    "#Convert dataset to np array\n",
    "features = data[['track_name','duration_ms','energy','danceability',\n",
    "                 'loudness','valence','tempo','time_signature','pitch_avg', \n",
    "                 'timbre_avg','key_change_percentage','mode_avg']].to_numpy()\n",
    "\n",
    "target = rating['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some dataset reformatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_features = features\n",
    "\n",
    "# Substitute titles with integers\n",
    "for i in range(cleaned_features.shape[0]):\n",
    "    cleaned_features[i][0] = i\n",
    "\n",
    "#convert the timbre and pitch vectors, which are actually strings in the dataset imported, to lists.\n",
    "for row in range(cleaned_features.shape[0]):\n",
    "    for col in range(cleaned_features.shape[1]):\n",
    "        if type(cleaned_features[row][col]) == str:\n",
    "            cleaned_features[row][col] = ast.literal_eval(cleaned_features[row][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The do a little \"hack\" to unpack the lists within the feature array,\n",
    "#and subesequently extent the coloumn size of the feature array.\n",
    "def flatten(x):\n",
    "    for item in x:\n",
    "        try:\n",
    "            yield from flatten(item) #if x has a member (item) it means its a a list or array, therefore we feed the item back into the function.\n",
    "        except TypeError: #so if x has no members to iterate on (i.e its a float or integer), we return it (yield)\n",
    "            yield item\n",
    "\n",
    "#the list features are actually imported as string, so we need to convert them back\n",
    "array = np.empty([])\n",
    "for i in range(cleaned_features.shape[0]):\n",
    "    row = list(flatten(cleaned_features[i]))\n",
    "    row = [round(elem, 2) for elem in row]\n",
    "    row = np.array(row)\n",
    "    if i == 0:\n",
    "        array = row\n",
    "    else:\n",
    "        array = np.vstack((array, row))\n",
    "\n",
    "cleaned_features = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000e+00  2.19947e+05  2.00000e-02  1.70000e-01 -2.78800e+01\n",
      "  7.00000e-02  1.35470e+02  4.00000e+00  3.40000e-01  2.40000e-01\n",
      "  3.20000e-01  1.40000e-01  3.10000e-01  2.40000e-01  1.40000e-01\n",
      "  2.90000e-01  1.30000e-01  3.90000e-01  2.60000e-01  1.40000e-01\n",
      "  2.70900e+01 -2.55000e+00  4.36200e+01 -1.15400e+01  4.74600e+01\n",
      " -3.23900e+01  5.96000e+00  1.30000e+00  9.31000e+00  5.99000e+00\n",
      " -9.92000e+00 -8.05000e+00  9.00000e+01  6.00000e-01]\n",
      "**************\n",
      "(100, 34)\n",
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     7\n",
       "1     8\n",
       "2     7\n",
       "3     8\n",
       "4     8\n",
       "     ..\n",
       "95    3\n",
       "96    3\n",
       "97    4\n",
       "98    4\n",
       "99    3\n",
       "Name: rating, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cleaned_features[0])\n",
    "print('**************')\n",
    "print(cleaned_features.shape)\n",
    "print(target.shape)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find how many components you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "componenets needed:  2\n",
      "reached:  0.9999996347104886 %\n"
     ]
    }
   ],
   "source": [
    "#Keep 99% of the variance from the original features\n",
    "\n",
    "feat_variance = np.var(cleaned_features, axis=0).sum()\n",
    "for i in range(cleaned_features.shape[1]):\n",
    "        temp = np.var(cleaned_features[:,0:i+1], axis=0).sum()\n",
    "        percentage = temp/feat_variance\n",
    "        if percentage > 0.99:\n",
    "            print(\"componenets needed: \", i+1)\n",
    "            print(\"reached: \", percentage, \"%\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda.fit(cleaned_features, target)\n",
    "projected_features = lda.transform(cleaned_features)\n",
    "\n",
    "#joblib_file = \"dimred.pkl\"\n",
    "#joblib.dump(lda, joblib_file)\n",
    "\n",
    "print(projected_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(projected_features)\n",
    "\n",
    "#joblib_file = \"scaler.pkl\"\n",
    "#joblib.dump(scaler, joblib_file)\n",
    "\n",
    "projected_features = sklearn.preprocessing.scale(projected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best set of parameters {'activation': 'logistic', 'hidden_layer_sizes': [2, 2], 'max_iter': 10000}\n",
      "associated best score 0.7492433337451194\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "crossval = RepeatedKFold(n_splits=5, n_repeats=20)\n",
    "params = {\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [[1], [2], [2, 3, 2], [2,3,4,3,2], [2, 2]],\n",
    "    'max_iter': [2000, 5000, 10000, 20000]\n",
    "}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=mlp,\n",
    "                     param_grid=params,\n",
    "                     scoring='r2',\n",
    "                     cv=crossval,  \n",
    "                     n_jobs=-1) \n",
    "\n",
    "gd_sr.fit(projected_features, target)\n",
    "\n",
    "print('best set of parameters', gd_sr.best_params_)\n",
    "print('associated best score',gd_sr.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1.3170\n",
      "Mean absolute error: 0.9543\n",
      "Median absolute error: 0.9196\n",
      "Coefficient of determination (r2 score): 0.7631\n",
      "Explained variance score: 0.7632\n",
      "r2 score on individual targets 0.7631341161606593\n"
     ]
    }
   ],
   "source": [
    "feat_train, feat_test, tar_train, tar_test = train_test_split(projected_features, target, test_size=0.2, random_state=5)\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(2, 2), max_iter=20000, activation='logistic', verbose=False)\n",
    "mlp.fit(feat_train, tar_train)\n",
    "tar_pred = mlp.predict(feat_test)\n",
    "\n",
    "#joblib_file = \"model.pkl\"\n",
    "#joblib.dump(mlp, joblib_file)\n",
    "\n",
    "\n",
    "print('Mean squared error: %.4f'% sklearn.metrics.mean_squared_error(tar_test, tar_pred))\n",
    "print('Mean absolute error: %.4f'% sklearn.metrics.mean_absolute_error(tar_test, tar_pred))\n",
    "print('Median absolute error: %.4f'% sklearn.metrics.median_absolute_error(tar_test, tar_pred))\n",
    "print('Coefficient of determination (r2 score): %.4f'% sklearn.metrics.r2_score(tar_test, tar_pred))\n",
    "print('Explained variance score: %.4f'% sklearn.metrics.explained_variance_score(tar_test, tar_pred))\n",
    "print('r2 score on individual targets',sklearn.metrics.r2_score(tar_test, tar_pred, multioutput='variance_weighted') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE mean and variance 1.2198372801317818 0.2512870008910244\n",
      "R2 mean and variance 0.7951102268777557 0.0004786477643060768 \n",
      "\n",
      "R2 mean difference from test and training set:  0.05399342385619638\n",
      "MSE mean difference from test and training set:  0.20736568624666055\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(2, 2), max_iter=20000, activation='logistic', verbose=False)\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=20)\n",
    "scores = cross_validate(mlp, projected_features, target, cv=cv ,scoring=('neg_mean_squared_error', 'r2'),return_train_score=True)\n",
    "\n",
    "print('MSE mean and variance', abs(np.mean(scores['test_neg_mean_squared_error'])),abs(np.var(scores['test_neg_mean_squared_error'])))\n",
    "print('R2 mean and variance', np.mean(scores['train_r2']),np.var(scores['train_r2']),'\\n')\n",
    "\n",
    "#Compute the fitting degree\n",
    "print('R2 mean difference from test and training set: ', np.mean(scores['train_r2']-scores['test_r2']))\n",
    "print('MSE mean difference from test and training set: ', abs(np.mean(scores['train_neg_mean_squared_error']-scores['test_neg_mean_squared_error'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add various amounts of Noise to do further testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 mean and variance of feat_ext (normal method):  0.7444465404840893 0.01158962702661096\n",
      "R2 mean and variance of train:  0.7941197249031795 0.0005552083690935509\n",
      "R2 mean difference:  0.04967318441909019\n",
      "Mean of all the differences between the R2 scores:  0.10016554108837107 \n",
      "\n",
      "MSE mean and variance of feat_ext (normal method):  1.2224452029915458 0.3075329449669512\n",
      "MSE mean and variance of train:  1.0181210607000801 0.014921189318534596\n",
      "MSE mean difference:  0.20432414229146567\n",
      "Mean of all the differences between the MSE scores:  0.5425875030647367 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#An ext_factor (extension factor) of 1 will add 100% more noisy training data, if 2 then 50% is added, etc..\n",
    "ext_factor = 1\n",
    "split = 5\n",
    "size_of_feat_train = int((projected_features.shape[0]/split)*(split-1))\n",
    "size_of_feat_train = round(size_of_feat_train/ext_factor)\n",
    "\n",
    "r2_score_feat_ext = np.array([])\n",
    "r2_score_train = np.array([])\n",
    "mse_score_feat_ext = np.array([])\n",
    "mse_score_train = np.array([])\n",
    "\n",
    "#shuffle the rows of two np.arrays in unison\n",
    "def unison_shuffle(x, y):\n",
    "    if len(x) == len(y):\n",
    "        p = np.random.permutation(len(x))\n",
    "        return x[p], y[p]\n",
    "    else:\n",
    "        print('inputs to unison_shuffle have different lengths..')\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(2, 2), max_iter=20000, activation='logistic', verbose=False)\n",
    "rkf = RepeatedKFold(n_splits=split, n_repeats=20)\n",
    "\n",
    "for train_index, test_index in rkf.split(projected_features, target):\n",
    "    #creating splits manually\n",
    "    feat_train, feat_test = projected_features[train_index], projected_features[test_index]\n",
    "    tar_train, tar_test = target[train_index], target[test_index]\n",
    "    \n",
    "    #extending by adding noise\n",
    "    feat_train_ext = np.append(feat_train, feat_train[:size_of_feat_train,:] + np.random.normal(0, 0.1, (feat_train[:size_of_feat_train,:].shape)), axis=0)\n",
    "    tar_train_ext = np.append(tar_train, tar_train[:size_of_feat_train], axis=0)\n",
    "    \n",
    "    #Shuffling the extended features and labels in unison\n",
    "    feat_train_ext, tar_train_ext = unison_shuffle(feat_train_ext, tar_train_ext) \n",
    "\n",
    "    #training and testing\n",
    "    mlp.fit(feat_train_ext, tar_train_ext)\n",
    "    tar_predict_feat_ext = mlp.predict(feat_test)\n",
    "    tar_predict_train = mlp.predict(feat_train)\n",
    "    \n",
    "    #Add the scores to arrays MAKE R2 or neg mean squared error\n",
    "    r2_score_feat_ext = np.append(r2_score_feat_ext, sklearn.metrics.r2_score(tar_test, tar_predict_feat_ext))\n",
    "    r2_score_train = np.append(r2_score_train, sklearn.metrics.r2_score(tar_train, tar_predict_train))\n",
    "    mse_score_feat_ext = np.append(mse_score_feat_ext, sklearn.metrics.mean_squared_error(tar_test, tar_predict_feat_ext))\n",
    "    mse_score_train = np.append(mse_score_train, sklearn.metrics.mean_squared_error(tar_train, tar_predict_train))\n",
    "\n",
    "\n",
    "#plotting the loss curve over training iteration \n",
    "#plt.plot(mlp.loss_curve_)\n",
    "#plt.xlabel('iteration')\n",
    "#plt.ylabel('loss')\n",
    "#plt.show()    \n",
    "\n",
    "r2_collected_score_diff = np.array([])\n",
    "for i in range(r2_score_feat_ext.size-1):\n",
    "    r2_collected_score_diff = np.append(r2_collected_score_diff, abs(r2_score_feat_ext[i]-r2_score_train[i]))\n",
    "\n",
    "print('R2 mean and variance of feat_ext (normal method): ', np.mean(r2_score_feat_ext), np.var(r2_score_feat_ext))\n",
    "print('R2 mean and variance of train: ', np.mean(r2_score_train),np.var(r2_score_train))\n",
    "print('R2 mean difference: ', abs(np.mean(r2_score_feat_ext)-np.mean(r2_score_train)))\n",
    "print('Mean of all the differences between the R2 scores: ', np.mean(r2_collected_score_diff),'\\n')\n",
    "\n",
    "mse_collected_score_diff = np.array([])\n",
    "for i in range(mse_score_feat_ext.size-1):\n",
    "    mse_collected_score_diff = np.append(mse_collected_score_diff, abs(mse_score_feat_ext[i]-mse_score_train[i]))\n",
    "\n",
    "print('MSE mean and variance of feat_ext (normal method): ', np.mean(mse_score_feat_ext), np.var(mse_score_feat_ext))\n",
    "print('MSE mean and variance of train: ', np.mean(mse_score_train),np.var(mse_score_train))\n",
    "print('MSE mean difference: ', abs(np.mean(mse_score_feat_ext)-np.mean(mse_score_train)))\n",
    "print('Mean of all the differences between the MSE scores: ', np.mean(mse_collected_score_diff),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
